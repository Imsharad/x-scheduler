# X Automation

A minimal Python-based Twitter/X automation pipeline that posts tweets from a simple CSV file.

## Features

- **Minimalist Design**: Extremely lean codebase focused on the essentials
- **CSV Content Source**: Uses a simple CSV file with two columns (tweet, is_posted)
- **Direct Posting**: Posts tweets without additional formatting
- **Flexible Scheduling**: Post at regular intervals or specific times
- **API Compliant**: Respects Twitter API rate limits and terms of service

## Project Structure

```
x-scheduler/
├── src/                     # Core source code
│   ├── __init__.py
│   ├── main.py              # Main execution script
│   ├── config_loader.py     # Loads config from .env and config file
│   ├── content_sources/     # Module for different content sources
│   │   ├── __init__.py
│   │   ├── base_source.py   # Abstract base class for sources
│   │   ├── rss_source.py    # Implementation for RSS feeds
│   │   └── file_source.py   # Implementation for curated file source
│   ├── content_processor.py # Handles templating, hashtagging, CTAs, UTMs
│   ├── scheduler.py         # Sets up and runs the scheduling logic
│   ├── twitter_poster.py    # Handles interaction with the X API
│   └── utils.py             # Utility functions (logging, retry logic)
├── config/                  # Configuration files
│   ├── config.yaml          # Main configuration
│   └── .env.example         # Example environment variables file
├── data/                    # Data files used/generated by the pipeline
│   ├── curated_content.csv  # Example curated content file
│   └── processed_log.txt    # Log of processed items to avoid duplicates
├── logs/                    # Log files generated by the application
│   └── pipeline.log         # Main log file
├── deploy/                  # Deployment-related files
│   ├── README.md            # Deployment-specific documentation
│   └── aws_policy.json      # AWS IAM policy templates
├── tests/                   # Unit/Integration tests
├── .gitignore               # Git ignore file
├── README.md                # Project documentation
├── requirements.txt         # Python dependencies (pinned versions)
├── deploy.sh                # AWS deployment script
└── run.sh                   # Local execution script
```

## Local Setup

1. Clone the repository
2. Create a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
4. Create a `.env` file in the project root with your Twitter API credentials:
   ```
   X_API_KEY=YOUR_API_KEY_HERE
   X_API_KEY_SECRET=YOUR_API_KEY_SECRET_HERE
   X_ACCESS_TOKEN=YOUR_ACCESS_TOKEN_HERE
   X_ACCESS_TOKEN_SECRET=YOUR_ACCESS_TOKEN_SECRET_HERE
   X_BEARER_TOKEN=YOUR_BEARER_TOKEN_HERE
   ```
5. Configure `src/cfg/config.yaml` to set up scheduling
6. Add content to `src/dat/curated_content.csv`

## AWS Deployment

For detailed AWS deployment instructions, see [deploy/README.md](deploy/README.md).

## Configuration

The `src/cfg/config.yaml` file controls the application:

```yaml
# Content source
content_csv_path: "src/dat/curated_content.csv"
content_format: "csv"

# Scheduling
schedule:
  mode: "interval"  # interval or specific_times
  interval_minutes: 240  # post every 4 hours
  specific_times:  # used when mode is specific_times
    - "09:00"
    - "17:00"
    - "21:00"

# Logging
logging:
  file_path: "src/log/pipeline.log"
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

## Content CSV Format

The curated content CSV file has a minimal structure:
- `tweet`: The content to be tweeted
- `is_posted`: Set to 'true' if already posted (updated automatically)

Example:
```csv
tweet,is_posted
Five Ways to Improve Your Workflow,false
The Future of AI in Marketing,false
```

## Local Usage

Run the pipeline locally:

```bash
./run.sh
# OR with debug logging:
./run.sh --debug
```

## Running Locally in the Background

### Linux/Mac:
```
nohup ./run.sh > /dev/null 2>&1 &
```

### Windows:
Use Task Scheduler to run the script at system startup.

## Important Notes

- This is a minimal project designed for simplicity
- Make sure your Twitter API credentials have the necessary permissions
- Always respect Twitter's terms of service and rate limits
- The AWS deployment uses the free tier for the first 12 months ($0), then costs approximately $8-12/month after that period 

## Google Sheets Integration

The X Scheduler now supports using a Google Sheet as the source for curated content, making content management easier and eliminating the need to update CSV files directly.

### Setup Google Sheets Integration

1. **Create a Google Sheet** with at least the following columns:
   - `tweet` - The tweet text to post
   - `is_posted` - Set to "true" for posted tweets, leave empty for unposted

2. **Create a Google Cloud Project and Service Account**:
   - Go to [Google Cloud Console](https://console.cloud.google.com/)
   - Create a new project (or use an existing one)
   - Enable the Google Sheets API for the project
   - Create a service account under "IAM & Admin > Service Accounts"
   - Create a key for the service account (JSON format)
   - Download and save the JSON key file to a secure location (e.g., `config/google-credentials.json`)

3. **Share Your Google Sheet** with the service account email (looks like `your-service-account@your-project.gserviceaccount.com`)

4. **Update Configuration**:
   - Set `content_source_type` to `google_sheet` in your `config.yaml`
   - Configure the `google_sheet` section with your Sheet ID, worksheet name (optional), and credentials path

   ```yaml
   content_source_type: "google_sheet"
   
   google_sheet:
     sheet_id: "your-google-sheet-id-here"
     worksheet_name: "Sheet1"  # Optional
     credentials_path: "config/google-credentials.json"
   ```

5. **Alternatively, set the credentials path via environment variable**:
   ```
   GOOGLE_CREDENTIALS_PATH=/path/to/your/credentials.json
   ```

### Security Considerations

- Never commit the Google service account credentials JSON file to your repository
- Add the credentials file path to your `.gitignore`
- Consider storing the credentials in AWS SSM Parameter Store if deploying to AWS 